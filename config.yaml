# ============================================================
# B2B SaaS Buying Intelligence Module — Pipeline Configuration
# ============================================================

project:
  name: "saas_buying_intelligence"
  version: "1.0.0"
  description: "Agent-ready intelligence module for B2B SaaS buying behavior"

# --- Ingestion ---
ingestion:
  # Common Crawl settings
  common_crawl:
    index: "CC-MAIN-2024-46"                    # Which crawl to query
    cdx_api_url: "https://index.commoncrawl.org"
    max_pages: 5                                 # CDX pages to fetch
    max_wet_files: 160                           # WET segments to download
    max_records_per_file: 50000                  # Cap records per WET file
    # Domain hints — CDX will filter to URLs matching these
    domain_filters:
      - "*.com"
    # Text must contain at least one of these to be retained at ingestion
    keyword_prefilter:
      - "SaaS"
      - "software purchase"
      - "vendor selection"
      - "procurement"
      - "RFP"
      - "software evaluation"
      - "cloud platform"
      - "enterprise software"

  # Optional licensed data ingestion
  licensed_data:
    input_dir: "data/licensed"
    accepted_formats: [".csv", ".json", ".jsonl"]

# --- Filtering ---
filtering:
  # Weighted keyword signals for SaaS buying behavior
  # Higher weights on decision verbs and narrative markers to surface
  # evaluation/selection/rejection stories vs generic product copy.
  signal_keywords:
    # Decision verbs (high weight — core signal)
    chose: 5
    choose: 4
    selected: 5
    shortlist: 4
    compare: 3
    evaluated: 5
    evaluate: 4
    adopted: 5
    rejected: 5
    replaced: 5
    switched: 5
    migrated: 5
    procured: 5
    purchased: 4
    trialed: 4
    piloted: 4
    # Lifecycle / procurement terms
    procurement: 5
    RFP: 5
    proof of concept: 5
    due diligence: 5
    vendor selection: 6
    security review: 5
    compliance check: 5
    vendor assessment: 5
    # Switching / migration signals
    moved from: 6
    switched from: 6
    migrated from: 6
    migrated away: 6
    replaced with: 5
    transitioned from: 5
    # Friction / objection signals
    vendor lock-in: 5
    too expensive: 5
    integration issues: 5
    scalability issues: 5
    security concerns: 5
    lacked support: 5
    onboarding difficulty: 5
    # Lifecycle terms
    onboarding: 3
    implementation: 3
    rollout: 3
    data migration: 4
    SLA: 4
    # General context
    integration: 2
    pricing: 3
    scalability: 2
    vendor: 4
    contract: 3
    renewal: 3
    pilot: 4
    stakeholder: 3
    compliance: 3
    ROI: 4
    total cost of ownership: 5
    single sign-on: 3
    API: 2
    trial: 3
    deploy: 2
    migrate: 4

  # Minimum score to pass filtering
  min_score: 8
  # Context window: sentences before/after match
  context_sentences_before: 1
  context_sentences_after: 1
  # Hard max for any stored text snippet (expanded to 480 to cover all 3 gate signals)
  max_snippet_chars: 480

# --- Extraction ---
extraction:
  # Confidence thresholds
  high_confidence_threshold: 0.7
  low_confidence_threshold: 0.4
  # LLM fallback (optional — set enabled: true and provide API key)
  llm_fallback:
    enabled: false
    provider: "anthropic"
    model: "claude-haiku-4-5-20251001"
    api_key_env: "ANTHROPIC_API_KEY"
    max_calls: 500                               # Budget cap
    cost_per_call_estimate: 0.002

# --- Validation ---
validation:
  # Fuzzy deduplication
  dedup:
    similarity_threshold: 90                     # 0-100, rapidfuzz ratio
    field: "decision_context"
  # PII detection — regex-based
  pii:
    drop_on_detect: true
    patterns:
      email: true
      phone: true
      address: true
      ssn: true
  # Maximum allowed snippet length in final output (matches extraction window)
  max_text_length: 480
  # Minimum confidence to include in final dataset
  # The 3-condition narrative gate (verb+reason+actor on full text) is the
  # primary quality enforcer. Confidence is a secondary structural signal.
  # Lowered to 0.35 so that real decision narratives with fewer structured
  # metadata fields (criteria/workflow/industry) are not discarded.
  min_confidence: 0.35

# --- Packaging ---
packaging:
  output_dir: "output"
  dataset_filename: "dataset.parquet"
  schema_filename: "schema.json"
  provenance_filename: "provenance.json"
  quality_report_filename: "quality_report.json"

# --- Embeddings ---
embeddings:
  model: "all-MiniLM-L6-v2"                     # sentence-transformers model
  batch_size: 64
  output_format: "npy"                           # npy or faiss_index
  dimension: 384

# --- RAG ---
rag:
  chunk_strategy: "row"                          # each row = one chunk
  similarity_metric: "cosine"
  top_k: 10
  similarity_threshold: 0.72
